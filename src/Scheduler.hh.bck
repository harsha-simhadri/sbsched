#ifndef __SCHEDULER_HH
#define __SCHEDULER_HH

#include "Job.hh"
#include "sync_queue.h"
#include <deque>
#include <vector>
#include <string>
#include <cmath>
#include <cstdlib>
#include <sys/types.h>
#include <linux/unistd.h>
#include <unistd.h>
#include <stdint.h>
#include <assert.h>

class Scheduler {
protected:
  int               _num_threads;                 // Number of threads (also num procs??)
  std::vector<Job*> _job_queue;                   // Jobs to be done
  Mutex             _queue_lock;
  pid_t *           _pthread_map;                  // Pthread ID map
public:
  Scheduler (int num_thr)
    : _num_threads (num_thr)//, _pool(NULL)
    {}
  bool check_range  (int id, int start, int end,
		     std::string * func_name);    // Check if the thread id is in proper range (start inclusive, end exclusive), print error and exit otherwise
  void set_pthread_map (pid_t *map) {_pthread_map=map;}
  
  virtual void add  (Job *job, int thread_id);    // Add a job to the task queue, -1 thread_id for anon enqueues
  virtual void done (Job *job, int thread_id,     // This job is done, -1 thread_id for anon calls
		     bool deactivate) {}          // Is this the end of the task of which the strand is a part
  virtual Job* get  (int thread_id=-1);           // Get a job. if more(x) returned TRUE, calling this immediately should return a job
  virtual bool more (int thread_id=-1);           // if arg=-1, check if any jobs in system,
                                                  // else, check if any jobs that can be handled by this thread
                                                  // implementations of derived classes should confirm to this
  virtual void print_scheduler_stats();
};

class WS_Scheduler : public Scheduler {
protected:
  int                 _num_jobs;                  // Total number of jobs
  int               * _num_steals;                // Number of steals, one counter for each job
  std::vector<Job*> * _job_queues;                // One queue per processor
  Mutex             * _local_lock;                // Local processor locks this before grabbing a locally queued job
  Mutex             * _steal_lock;                // Stealing procs grab this lock before locking the local lock
public:
  WS_Scheduler (int num_thr)
    : Scheduler (num_thr),
      _num_jobs (0) {
    _job_queues = new std::vector<Job*>[_num_threads];
    _local_lock = new Mutex[num_thr];
    _steal_lock = new Mutex[num_thr];
  }
  ~WS_Scheduler();
  
  void add  (Job *job, int thread_id );           // Add a job to the task queue, -1 thread_id for anon enqueues
  Job* get  (int thread_id=-1);                   // Get a job             
  bool more (int thread_id=-1);                
  void done (Job *job, int thread_id,
		     bool deactivate);
  void print_scheduler_stats();
};

class Local_Scheduler : public Scheduler {
protected:
  int                 _num_jobs;                  // Total number of jobs 
  std::vector<Job*> * _job_queues;                // One queue per processor
public:
  Local_Scheduler (int num_thr)
    : Scheduler (num_thr),
      _num_jobs (0) {
    _job_queues = new std::vector<Job*>[_num_threads];
  }
  ~Local_Scheduler () {
    delete _job_queues;
  }
  
  void add  (Job *job, int thread_id );           // Add a job to the task queue, -1 thread_id for anon enqueues
  Job* get  (int thread_id=-1);                   // Get a job             
  bool more (int thread_id=-1);                
  void done (Job *job, int thread_id,
		     bool deactivate);
  void print_scheduler_stats() {};
};

typedef struct TaskSet {
  friend class HR_Scheduler;

  volatile int               _clusters_needed;
  volatile int               _clusters_attached;
  
  bool              _complete;                   // Is this task complete?
  
  lluint            _parent_job_id;
  Fork *            _parent_fork;
  Mutex             _lock;
  
  std::deque<SizedJob*> _task_queue;
  
  TaskSet (Fork * parent_fork, lluint parent_job_id, int num)
    : _parent_fork (parent_fork),
      _parent_job_id (parent_job_id),
      _clusters_needed (num),
      _clusters_attached (0),
      _complete (false)
    {}
  
  int               change_clusters_needed(int delta) {
    return __sync_add_and_fetch(&_clusters_needed, delta); 
  }
  int               change_clusters_attached(int delta) {
    return __sync_add_and_fetch(&_clusters_attached, delta); 
  }
  void              lock   ()   {_lock.lock();}
    void              unlock ()   {_lock.unlock();}
} TaskSet;

struct Cluster;  
typedef struct Cluster {
  friend class TreeOfCaches;
  
  const lluint      _size;                      // In Bytes
  const uint        _block_size;                // In Bytes
  int               _num_children;              // No. of subclusters
  Cluster *         _parent;
  Cluster **        _children;
  


  lluint            _occupied;                  // Occupied size in byes
  
  TaskSet *         _active_set;                // A cluster can execute tasks only from this set
   
  TaskSet *         _spawned_set;               // Set of tasks pinned to a cluster
                                                // In other words, tasks can be executed only under this cluster
                                                // Sublcusters point to this set using their _active_set pointers.
  
  Mutex             _lock;
  
  Cluster (const lluint size, const int block_size, const int num_children,
	   Cluster * parent=NULL, Cluster ** chidren=NULL);
  
  Cluster *         create_tree ( Cluster * root, Cluster ** leaf_array,
				  int num_levels, uint * fan_outs,
				    lluint * sizes, uint * block_sizes);
  void              lock      () {_lock.lock();}
  void              unlock    () {_lock.unlock();}
  bool              is_locked () {return _lock.is_locked();}

  std::vector<TaskSet*> _spawned_set_vector;      // List of spawned sets that are currently active on subclusters
                                                  // Need lock on cluster to access
  
  // Added by Aapo
  int               _locked_thread_id;         // Thread that locked this cluster
  void set_active_set(TaskSet * a, int thrid) { 
    assert(thrid == _locked_thread_id);
    _active_set = a; 
  }
  
  
} Cluster;

class TreeOfCaches {
  friend class HR_Scheduler;

  int                 _num_levels;
  int                 _num_leaves;
  
  Cluster *           _root;
  Cluster **          _leaf_array;
  
  int     *           _num_locks_held;           // #locks held by the thread
  Cluster ***         _locked_clusters;         // There is one list for each thread,
                                                 // in the order in which they were obtained.
    
  lluint  *           _sizes;
  uint    *           _block_sizes;
  uint    *           _fan_outs;
};

class HR_Scheduler : public Scheduler {
protected:
  
  TreeOfCaches *      _tree;
  
  lluint              _num_jobs;

  int                 _type;                      // 0(def): Spawned set statically alocated to subclusters
                                                  // 1     : Active_set link can move to other spawned set under parent

  Cluster *           find_active_set (int thread_id, SizedJob* sized_job);
                                                  // Search from the leaves of the tree up until you
                                                  // find an active set to which sized_job is pinned
                                                  // and return it. Lock up all nodes till there.

  void                release_active_set (Cluster *node, int thrid);
                                                  // Require that node's active_set be locked,
                                                  // and that the active_set be completed.

  void                insert_task_in_queue (std::deque<SizedJob*> &task_queue, SizedJob *job, lluint block_size);
                                                  // Insert task in to the task queue of an active set
  
public:
  HR_Scheduler (int num_threads,                  // Threads are logically numbered left to right.
		int num_levels, int * fan_outs,   // num levels including top level RAM, f_{},
		lluint * sizes, int * block_sizes,// M_{}, B_{}; M_0 is neglected
		int type=0);
  ~HR_Scheduler ();
  
  int allocate (lluint size,
		lluint lower_cache_size,          // Set this to 0, if this is processor and upper is L1
		lluint upper_cache_size,
		int fan_out) {
    #define MIN(A, B) ((A<B)?A:B)
    return (lower_cache_size == 0
	    ? fan_out
	    : MIN (fan_out, (int)ceil (((double)size)/((double)lower_cache_size))));
    //: (int)ceil (fan_out*(((double)size)/((double)upper_cache_size))));
  }
  
 
  void add  (Job *job, int thread_id ); 
  void done (Job *job, int thread_id, bool deactivate );
  Job* get  (int thread_id=-1);
  bool more (int thread_id=-1);
  void print_scheduler_stats() {};
  Job* find_job (int thread_id, Cluster *node=NULL);

  void check_lock_consistency (int thread_id);
  
  void lock     (Cluster* node, int thread_id);
  bool has_lock (Cluster* node, int thread_id);
  void unlock   (Cluster* node, int thread_id);
  void print_locks   (int thread_id);
  void release_locks (int thread_id);
  
  void print_tree( Cluster * root, int num_levels, int total_levels=-1);
  void print_job ( SizedJob *job);
};
  
#endif
